{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.utils import to_categorical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the dataset\n",
    "data = pd.read_csv(\"dataset.csv\")\n",
    "\n",
    "# Preprocessing\n",
    "# Encode the label column\n",
    "label_encoder = LabelEncoder()\n",
    "data['label'] = label_encoder.fit_transform(data['label'])\n",
    "\n",
    "# Separate features and target\n",
    "X = data.drop(columns=['label']).values\n",
    "y = data['label'].values\n",
    "\n",
    "# Normalize the input features\n",
    "scaler = StandardScaler()\n",
    "X = scaler.fit_transform(X)\n",
    "\n",
    "# Convert labels to categorical (one-hot encoding)\n",
    "y = to_categorical(y)\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\harsh\\OneDrive\\Desktop\\Semester 7\\Internet of Things (CS539)\\Project\\.venv\\Lib\\site-packages\\keras\\src\\layers\\core\\dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.1587 - loss: 2.9917 - val_accuracy: 0.4517 - val_loss: 2.4149\n",
      "Epoch 2/50\n",
      "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.5659 - loss: 2.1082 - val_accuracy: 0.7330 - val_loss: 1.3662\n",
      "Epoch 3/50\n",
      "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.7866 - loss: 1.1013 - val_accuracy: 0.8523 - val_loss: 0.7139\n",
      "Epoch 4/50\n",
      "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.8899 - loss: 0.5945 - val_accuracy: 0.8977 - val_loss: 0.4712\n",
      "Epoch 5/50\n",
      "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.9328 - loss: 0.3867 - val_accuracy: 0.9148 - val_loss: 0.3497\n",
      "Epoch 6/50\n",
      "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.9498 - loss: 0.2820 - val_accuracy: 0.9261 - val_loss: 0.2737\n",
      "Epoch 7/50\n",
      "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.9558 - loss: 0.2296 - val_accuracy: 0.9489 - val_loss: 0.2252\n",
      "Epoch 8/50\n",
      "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.9673 - loss: 0.1756 - val_accuracy: 0.9489 - val_loss: 0.1902\n",
      "Epoch 9/50\n",
      "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.9684 - loss: 0.1526 - val_accuracy: 0.9517 - val_loss: 0.1657\n",
      "Epoch 10/50\n",
      "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.9771 - loss: 0.1328 - val_accuracy: 0.9574 - val_loss: 0.1553\n",
      "Epoch 11/50\n",
      "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9762 - loss: 0.1182 - val_accuracy: 0.9489 - val_loss: 0.1353\n",
      "Epoch 12/50\n",
      "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9850 - loss: 0.1028 - val_accuracy: 0.9602 - val_loss: 0.1201\n",
      "Epoch 13/50\n",
      "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.9698 - loss: 0.0990 - val_accuracy: 0.9801 - val_loss: 0.1124\n",
      "Epoch 14/50\n",
      "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.9837 - loss: 0.0964 - val_accuracy: 0.9773 - val_loss: 0.0982\n",
      "Epoch 15/50\n",
      "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.9801 - loss: 0.0794 - val_accuracy: 0.9773 - val_loss: 0.0946\n",
      "Epoch 16/50\n",
      "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.9799 - loss: 0.0708 - val_accuracy: 0.9801 - val_loss: 0.0834\n",
      "Epoch 17/50\n",
      "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.9868 - loss: 0.0606 - val_accuracy: 0.9744 - val_loss: 0.0818\n",
      "Epoch 18/50\n",
      "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.9896 - loss: 0.0568 - val_accuracy: 0.9716 - val_loss: 0.0797\n",
      "Epoch 19/50\n",
      "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.9850 - loss: 0.0618 - val_accuracy: 0.9716 - val_loss: 0.0800\n",
      "Epoch 20/50\n",
      "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.9893 - loss: 0.0495 - val_accuracy: 0.9688 - val_loss: 0.0783\n",
      "Epoch 21/50\n",
      "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.9857 - loss: 0.0516 - val_accuracy: 0.9688 - val_loss: 0.0873\n",
      "Epoch 22/50\n",
      "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.9888 - loss: 0.0442 - val_accuracy: 0.9688 - val_loss: 0.0708\n",
      "Epoch 23/50\n",
      "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9867 - loss: 0.0495 - val_accuracy: 0.9716 - val_loss: 0.0722\n",
      "Epoch 24/50\n",
      "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.9900 - loss: 0.0403 - val_accuracy: 0.9773 - val_loss: 0.0620\n",
      "Epoch 25/50\n",
      "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9903 - loss: 0.0404 - val_accuracy: 0.9830 - val_loss: 0.0566\n",
      "Epoch 26/50\n",
      "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.9919 - loss: 0.0368 - val_accuracy: 0.9744 - val_loss: 0.0627\n",
      "Epoch 27/50\n",
      "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.9947 - loss: 0.0327 - val_accuracy: 0.9801 - val_loss: 0.0581\n",
      "Epoch 28/50\n",
      "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.9914 - loss: 0.0364 - val_accuracy: 0.9716 - val_loss: 0.0605\n",
      "Epoch 29/50\n",
      "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.9897 - loss: 0.0401 - val_accuracy: 0.9830 - val_loss: 0.0526\n",
      "Epoch 30/50\n",
      "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.9907 - loss: 0.0314 - val_accuracy: 0.9886 - val_loss: 0.0516\n",
      "Epoch 31/50\n",
      "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.9915 - loss: 0.0302 - val_accuracy: 0.9688 - val_loss: 0.0595\n",
      "Epoch 32/50\n",
      "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.9911 - loss: 0.0331 - val_accuracy: 0.9716 - val_loss: 0.0624\n",
      "Epoch 33/50\n",
      "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9935 - loss: 0.0321 - val_accuracy: 0.9830 - val_loss: 0.0475\n",
      "Epoch 34/50\n",
      "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9933 - loss: 0.0207 - val_accuracy: 0.9830 - val_loss: 0.0507\n",
      "Epoch 35/50\n",
      "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.9932 - loss: 0.0251 - val_accuracy: 0.9688 - val_loss: 0.0573\n",
      "Epoch 36/50\n",
      "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.9940 - loss: 0.0288 - val_accuracy: 0.9830 - val_loss: 0.0459\n",
      "Epoch 37/50\n",
      "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.9922 - loss: 0.0259 - val_accuracy: 0.9830 - val_loss: 0.0482\n",
      "Epoch 38/50\n",
      "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9972 - loss: 0.0215 - val_accuracy: 0.9801 - val_loss: 0.0465\n",
      "Epoch 39/50\n",
      "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.9918 - loss: 0.0248 - val_accuracy: 0.9716 - val_loss: 0.0608\n",
      "Epoch 40/50\n",
      "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.9965 - loss: 0.0178 - val_accuracy: 0.9858 - val_loss: 0.0441\n",
      "Epoch 41/50\n",
      "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.9944 - loss: 0.0211 - val_accuracy: 0.9830 - val_loss: 0.0445\n",
      "Epoch 42/50\n",
      "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.9945 - loss: 0.0205 - val_accuracy: 0.9858 - val_loss: 0.0439\n",
      "Epoch 43/50\n",
      "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.9960 - loss: 0.0205 - val_accuracy: 0.9915 - val_loss: 0.0408\n",
      "Epoch 44/50\n",
      "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.9982 - loss: 0.0172 - val_accuracy: 0.9801 - val_loss: 0.0462\n",
      "Epoch 45/50\n",
      "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.9964 - loss: 0.0189 - val_accuracy: 0.9858 - val_loss: 0.0422\n",
      "Epoch 46/50\n",
      "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.9989 - loss: 0.0149 - val_accuracy: 0.9886 - val_loss: 0.0401\n",
      "Epoch 47/50\n",
      "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.9938 - loss: 0.0203 - val_accuracy: 0.9659 - val_loss: 0.0651\n",
      "Epoch 48/50\n",
      "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.9898 - loss: 0.0254 - val_accuracy: 0.9716 - val_loss: 0.0584\n",
      "Epoch 49/50\n",
      "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.9933 - loss: 0.0183 - val_accuracy: 0.9858 - val_loss: 0.0391\n",
      "Epoch 50/50\n",
      "\u001b[1m88/88\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9963 - loss: 0.0162 - val_accuracy: 0.9801 - val_loss: 0.0443\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9738 - loss: 0.0754 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy: 0.97\n"
     ]
    }
   ],
   "source": [
    "import h5py\n",
    "\n",
    "# Build and train the ANN model (as in your code)\n",
    "model = Sequential([\n",
    "    Dense(64, input_dim=X_train.shape[1], activation='relu'),  # Hidden layer 1\n",
    "    Dense(32, activation='relu'),  # Hidden layer 2\n",
    "    Dense(y_train.shape[1], activation='softmax')  # Output layer\n",
    "])\n",
    "\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "history = model.fit(X_train, y_train, epochs=50, batch_size=16, validation_split=0.2, verbose=1)\n",
    "\n",
    "# Evaluate the model\n",
    "test_loss, test_accuracy = model.evaluate(X_test, y_test, verbose=1)\n",
    "print(f\"Test Accuracy: {test_accuracy:.2f}\")\n",
    "\n",
    "# Save the trained model\n",
    "model.save(\"model.h5\")\n",
    "\n",
    "# Append the LabelEncoder and scaler attributes to the same `.h5` file\n",
    "with h5py.File(\"model.h5\", \"a\") as f:\n",
    "    # Save LabelEncoder classes\n",
    "    f.create_dataset(\"label_encoder_classes\", data=label_encoder.classes_)\n",
    "    \n",
    "    # Save scaler mean and scale\n",
    "    f.create_dataset(\"scaler_mean\", data=scaler.mean_)\n",
    "    f.create_dataset(\"scaler_scale\", data=scaler.scale_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 55ms/step\n",
      "Rank 1: coffee (Probability: 0.73)\n",
      "Rank 2: jute (Probability: 0.23)\n",
      "Rank 3: watermelon (Probability: 0.02)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tensorflow.keras.models import load_model\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "\n",
    "# Load the trained model\n",
    "model = load_model(\"model.h5\")\n",
    "\n",
    "# Load preprocessing objects (LabelEncoder and scaler) from the `.h5` file\n",
    "with h5py.File(\"model.h5\", \"r\") as f:\n",
    "    # Load LabelEncoder classes\n",
    "    label_encoder = LabelEncoder()\n",
    "    label_encoder.classes_ = f[\"label_encoder_classes\"][:]\n",
    "    \n",
    "    # Load scaler mean and scale\n",
    "    scaler = StandardScaler()\n",
    "    scaler.mean_ = f[\"scaler_mean\"][:]\n",
    "    scaler.scale_ = f[\"scaler_scale\"][:]\n",
    "\n",
    "# Example new data (N, P, K, temperature, humidity, pH, rainfall)\n",
    "new_data = np.array([[100, 30, 40, 26.5, 75.0, 6.5, 120]])\n",
    "\n",
    "# Preprocess the input data (scale it)\n",
    "new_data_scaled = scaler.transform(new_data)\n",
    "\n",
    "# # Make predictions\n",
    "predictions = model.predict(new_data_scaled)\n",
    "\n",
    "# Get the indices of the top 3 probabilities\n",
    "top_3_indices = np.argsort(predictions, axis=1)[:, -3:][:, ::-1]  # Sort and reverse to get top 3\n",
    "\n",
    "# Decode the top 3 indices into crop labels and convert byte strings to regular strings\n",
    "top_3_labels = [label.decode('utf-8') for label in label_encoder.inverse_transform(top_3_indices[0])]\n",
    "\n",
    "# Get the corresponding probabilities for the top 3 labels\n",
    "top_3_probabilities = predictions[0, top_3_indices[0]]\n",
    "\n",
    "# Print the results\n",
    "for i, (label, prob) in enumerate(zip(top_3_labels, top_3_probabilities)):\n",
    "    print(f\"Rank {i+1}: {label} (Probability: {prob:.2f})\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
